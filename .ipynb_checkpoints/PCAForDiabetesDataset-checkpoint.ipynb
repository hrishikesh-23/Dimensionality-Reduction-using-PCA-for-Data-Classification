{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bcb3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e9b5541",
   "metadata": {},
   "outputs": [],
   "source": [
    "relativeFilePath = 'DataSet/Diabetes.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e2a8f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(relativeFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32184c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ab3a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
       "       'BMI', 'DiabetesPedigreeFunction', 'Age'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = data[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']]\n",
    "labels = data[['Outcome']]\n",
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd9a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(features)\n",
    "scaledFeatures = scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a70c85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.639947</td>\n",
       "      <td>0.848324</td>\n",
       "      <td>0.149641</td>\n",
       "      <td>0.907270</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>0.204013</td>\n",
       "      <td>0.468492</td>\n",
       "      <td>1.425995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-1.123396</td>\n",
       "      <td>-0.160546</td>\n",
       "      <td>0.530902</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-0.684422</td>\n",
       "      <td>-0.365061</td>\n",
       "      <td>-0.190672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.233880</td>\n",
       "      <td>1.943724</td>\n",
       "      <td>-0.263941</td>\n",
       "      <td>-1.288212</td>\n",
       "      <td>-0.692891</td>\n",
       "      <td>-1.103255</td>\n",
       "      <td>0.604397</td>\n",
       "      <td>-0.105584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.844885</td>\n",
       "      <td>-0.998208</td>\n",
       "      <td>-0.160546</td>\n",
       "      <td>0.154533</td>\n",
       "      <td>0.123302</td>\n",
       "      <td>-0.494043</td>\n",
       "      <td>-0.920763</td>\n",
       "      <td>-1.041549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.141852</td>\n",
       "      <td>0.504055</td>\n",
       "      <td>-1.504687</td>\n",
       "      <td>0.907270</td>\n",
       "      <td>0.765836</td>\n",
       "      <td>1.409746</td>\n",
       "      <td>5.484909</td>\n",
       "      <td>-0.020496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
       "0     0.639947  0.848324       0.149641       0.907270 -0.692891  0.204013   \n",
       "1    -0.844885 -1.123396      -0.160546       0.530902 -0.692891 -0.684422   \n",
       "2     1.233880  1.943724      -0.263941      -1.288212 -0.692891 -1.103255   \n",
       "3    -0.844885 -0.998208      -0.160546       0.154533  0.123302 -0.494043   \n",
       "4    -1.141852  0.504055      -1.504687       0.907270  0.765836  1.409746   \n",
       "\n",
       "   DiabetesPedigreeFunction       Age  \n",
       "0                  0.468492  1.425995  \n",
       "1                 -0.365061 -0.190672  \n",
       "2                  0.604397 -0.105584  \n",
       "3                 -0.920763 -1.041549  \n",
       "4                  5.484909 -0.020496  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledFeatureDf = pd.DataFrame(data = scaledFeatures, columns = features.columns)\n",
    "scaledFeatureDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3130bc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2611c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#singular vectors using eigen values of Xtranspos*X\n",
    "# Xtranspose = scaledFeatures.transpose()\n",
    "# correlationMatrix = np.matmul(Xtranspose,scaledFeatures)\n",
    "# eigenValues, eigenVectors = np.linalg.eig(correlationMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5eaf2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leftSingularVectors,singularValues,rightSingularVectors = np.linalg.svd(scaledFeatures, full_matrices=True)\n",
    "original = np.dot(leftSingularVectors[:,:8]*singularValues,rightSingularVectors)\n",
    "np.allclose(original,scaledFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f9d5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transposedRSV = rightSingularVectors.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60f26fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaDataFrame = pd.DataFrame()\n",
    "pcaDataFrame['FirstComp'] = transposedRSV[0]\n",
    "pcaDataFrame['SecondComp'] = transposedRSV[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "976128d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "twoDData = np.dot(scaledFeatures,pcaDataFrame.to_numpy())\n",
    "colors = {0:'red',1:'blue'}\n",
    "plt.scatter(twoDData[:,0], twoDData[:,1],s=3, c = labels['Outcome'].map(colors))\n",
    "y = labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbd7ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(twoDData, y, train_size=0.8, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d1c5448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ceb28dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLossAndGradient(w,x,y,regularizer):\n",
    "    numberOfDataPoints = x.shape[0]\n",
    "    xw = np.dot(x,w)\n",
    "    loss = np.dot((y-xw).T,(y-xw)).item()/2*numberOfDataPoints\n",
    "    grad = (np.dot(x.T, (y-xw)))/numberOfDataPoints - regularizer*w\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d8e99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepestDescentLineSearch(w, x, y, regularizer,epsilon = 1e-07, max_epochs=500, gamma = 1e-07):\n",
    "    loss, grad = getLossAndGradient(w,x,y,regularizer)\n",
    "    numberOfEpochs = 0\n",
    "    while(numberOfEpochs<max_epochs and np.linalg.norm(grad)>epsilon):\n",
    "        learningRate = 1\n",
    "        w_new = w + learningRate*grad\n",
    "        print(\"Iteration : \",numberOfEpochs,\" w : \",w,\" Loss : \",loss)\n",
    "        new_loss, new_grad = getLossAndGradient(w_new,x,y,regularizer)\n",
    "        while(new_loss>loss - gamma*learningRate*np.dot(grad.T,grad)):\n",
    "            learningRate = learningRate/2\n",
    "            w_new = w + learningRate*grad\n",
    "            new_loss, new_grad = getLossAndGradient(w_new,x,y,regularizer)\n",
    "        w = w_new\n",
    "        print('learning rate used : ',learningRate)\n",
    "        loss = new_loss\n",
    "        grad = new_grad\n",
    "        numberOfEpochs += 1\n",
    "    print(\"total epochs needed : \",numberOfEpochs)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89d2f431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  0  w :  [1 1]  Loss :  362492.98425137554\n",
      "learning rate used :  0.5\n",
      "Iteration :  1  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  4.440892098500626e-16\n",
      "Iteration :  2  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  4.440892098500626e-16\n",
      "Iteration :  3  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  4  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  5  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  6  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  7  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  8  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  9  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  10  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  11  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  12  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  13  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  14  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  15  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  16  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  17  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  18  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  19  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  20  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  21  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  22  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  23  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  24  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  25  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  26  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  27  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  28  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  29  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  30  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  31  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  32  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  33  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  34  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  35  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  36  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  37  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  38  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  39  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  40  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  41  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  42  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  43  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  44  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  45  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  46  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  47  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  48  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  49  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  50  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  51  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  52  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  53  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  54  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  55  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  56  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  57  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  58  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  59  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  60  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  61  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  62  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  63  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  64  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  65  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  66  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  67  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  68  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  69  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  70  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  71  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  72  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  73  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  74  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  75  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  76  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  77  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  78  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  79  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  80  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  81  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  82  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  83  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  84  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  85  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  86  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  87  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  88  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  89  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  90  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  91  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  92  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  93  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  94  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  95  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  96  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  97  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  98  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  99  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  100  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  101  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  102  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  103  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  104  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  105  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  106  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  107  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  108  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  109  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  110  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  111  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  112  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  113  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  114  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  115  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  116  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  117  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  118  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  119  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  120  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  121  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  122  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  123  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  124  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  125  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  126  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  127  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  128  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  129  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  130  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  131  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  132  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  133  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  134  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  135  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  136  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  137  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  138  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  139  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  140  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  141  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  142  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  143  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  144  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  145  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  146  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  147  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  148  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  149  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  150  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  151  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  152  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  153  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  154  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  155  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  156  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  157  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  158  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  159  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  160  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  161  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  162  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  163  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  164  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  165  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  166  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  167  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  168  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  169  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  170  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  171  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  172  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  173  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  174  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  175  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  176  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  177  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  178  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  179  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  180  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  181  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  182  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  183  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  184  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  185  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  186  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  187  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  188  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  189  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  190  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  191  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  192  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  193  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  194  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  195  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  196  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  197  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  198  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  199  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  200  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  201  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  202  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  203  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  204  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  205  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  206  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  207  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  208  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  209  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  210  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  211  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  212  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  213  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  214  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  215  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  216  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  217  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  218  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  219  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  220  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  221  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  222  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  223  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  224  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  225  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  226  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  227  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  228  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  229  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  230  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  231  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  232  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  233  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  234  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  235  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  236  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  237  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  238  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  239  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  240  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  241  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  242  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  243  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  244  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  245  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  246  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  247  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  248  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  249  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  250  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  251  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  252  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  253  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  254  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  255  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  256  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  257  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  258  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  259  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  260  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  261  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  262  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  263  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  264  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  265  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  266  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  267  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  268  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  269  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  270  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  271  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  272  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  273  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  274  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  275  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  276  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  277  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  278  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  279  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  280  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  281  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  282  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  283  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  284  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  285  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  286  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  287  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  288  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  289  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  290  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  291  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  292  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  293  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  294  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  295  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  296  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  297  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  298  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  299  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  300  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  301  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  302  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  303  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  304  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  305  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  306  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  307  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  308  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  309  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  310  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  311  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  312  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  313  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  314  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  315  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  316  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  317  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  318  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  319  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  320  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  321  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  322  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  323  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  324  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  325  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  326  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  327  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  328  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  329  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  330  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  331  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  332  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  333  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  334  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  335  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  336  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  337  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  338  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  339  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  340  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  341  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  342  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  343  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  344  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  345  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  346  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  347  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  348  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  349  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  350  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  351  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  352  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  353  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  354  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  355  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  356  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  357  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  358  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  359  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  360  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  361  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  362  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  363  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  364  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  365  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  366  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  367  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  368  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  369  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  370  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  371  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  372  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  373  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  374  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  375  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  376  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  377  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  378  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  379  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  380  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  381  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  382  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  383  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  384  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  385  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  386  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  387  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  388  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  389  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  390  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  391  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  392  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  393  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  394  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  395  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  396  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  397  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  398  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  399  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  400  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  401  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  402  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  403  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  404  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  405  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  406  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  407  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  408  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  409  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  410  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  411  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  412  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  413  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  414  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  415  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  416  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  417  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  418  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  419  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  420  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  421  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  422  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  423  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  424  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  425  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  426  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  427  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  428  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  429  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  430  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  431  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  432  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  433  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  434  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  435  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  436  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  437  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  438  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  439  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  440  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  441  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  442  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  443  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  444  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  445  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  446  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  447  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  448  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  449  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  450  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  451  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  452  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  453  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  454  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  455  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  456  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  457  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  458  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  459  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  460  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  461  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  462  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  463  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  464  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  465  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  466  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  467  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  468  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  469  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  470  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  471  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  472  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  473  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  474  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  475  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  476  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  477  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  478  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  479  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  480  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  481  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  482  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  483  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  484  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  485  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  486  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  487  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  488  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  489  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  490  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  491  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  492  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  493  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  494  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  495  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  496  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  497  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  498  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "Iteration :  499  w :  [0.11333989 0.06589595]  Loss :  59344.12800509893\n",
      "learning rate used :  1.1102230246251565e-16\n",
      "total epochs needed :  500\n"
     ]
    }
   ],
   "source": [
    "w_learned = steepestDescentLineSearch(np.array([1,1]),x_train, y_train[:,0], 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9915d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-fold cross validation\n",
    "\n",
    "numberOfFolds = 5\n",
    "totalDataPoints = x_train.shape[0]\n",
    "DataPointsInOneFold = totalDataPoints//numberOfFolds\n",
    "foldStartIndex = np.zeros(numberOfFolds)\n",
    "foldEndIndex = np.zeros(numberOfFolds)\n",
    "\n",
    "for i in range(0,numberOfFolds):\n",
    "    foldName = 'fold'+str(1)\n",
    "    fsi = i*DataPointsInOneFold\n",
    "    fei = (i+1)*DataPointsInOneFold - 1\n",
    "    if i==numberOfFolds-1:\n",
    "        fei = totalDataPoints-1\n",
    "    foldStartIndex[i] = fsi\n",
    "    foldEndIndex[i] = fei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d25a1eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2278038 , -0.19084503],\n",
       "       [-0.81943324, -0.12444706],\n",
       "       [ 0.0579723 ,  0.1891487 ],\n",
       "       [-0.41242351,  0.67616162]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((x_train[0:0,:],x_train[4:4,:]),axis=0)\n",
    "x_train[0:4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5be9f8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41242351,  0.67616162],\n",
       "       [ 0.60236   ,  1.60296423]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData = x_train[0:0,:]\n",
    "np.concatenate((trainingData,x_train[3:5,:]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "102a6938",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validationData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21904\\3722500103.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0mtrainingData_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfoldStartIndex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meachFold\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfoldEndIndex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meachFold\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[0mvalidationData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidationData\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfoldStartIndex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meachFold\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfoldEndIndex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meachFold\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'validationData' is not defined"
     ]
    }
   ],
   "source": [
    "rangeOfRegularizer = np.arange(-20,20,0.5)\n",
    "bestReg = rangeOfRegularizer[0]\n",
    "maxValidationAcc = 0\n",
    "for reg in rangeOfRegularizer:\n",
    "    accuracyOfReg = 0\n",
    "    for testFold in range(0,numberOfFolds):\n",
    "        trainingData_x = x_train[0:0,:]\n",
    "        trainingData_y = y_train[0:0,:]\n",
    "        validationData_x = x_train[0:0,:]\n",
    "        validationData_y = y_train[0:0,:]\n",
    "        for eachFold in range(0,numberOfFolds):\n",
    "            if eachFold!=testFold:\n",
    "                trainingData_x = np.concatenate((trainingData_x,x_train[foldStartIndex[eachFold]:foldEndIndex[eachFold],:]),axis=0)\n",
    "                trainingData_y = np.concatenate((trainingData_y,y_train[foldStartIndex[eachFold]:foldEndIndex[eachFold],:]),axis=0)\n",
    "            else:\n",
    "                validationData_x = np.concatenate((validationData_x,x_train[foldStartIndex[eachFold]:foldEndIndex[eachFold],:]),axis=0)\n",
    "                validationData_y = np.concatenate((validationData_y,y_train[foldStartIndex[eachFold]:foldEndIndex[eachFold],:]),axis=0)\n",
    "        \n",
    "        w_learned = steepestDescentLineSearch(np.array([2,3]),x_train, y_train[:,0], 0.9)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ecfc5fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "122*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25edc55a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
